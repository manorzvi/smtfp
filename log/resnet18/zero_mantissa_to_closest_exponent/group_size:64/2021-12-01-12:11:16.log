2021-12-01 12:11:16.600 | INFO     | __main__:main:152 - Log into log/resnet18/zero_mantissa_to_closest_exponent/group_size:64/2021-12-01-12:11:16.log
2021-12-01 12:11:16.600 | WARNING  | __main__:main:158 - You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
2021-12-01 12:11:16.600 | INFO     | __main__:main_worker:179 - Using pre-trained model 'resnet18'
2021-12-01 12:11:16.741 | WARNING  | __main__:main_worker:186 - Replacing model's last FC layer by torch.nn.Linear(512, 10)
2021-12-01 12:11:16.742 | INFO     | __main__:main_worker:188 - 
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
2021-12-01 12:11:16.743 | WARNING  | __main__:main_worker:195 - Using CPU, this will be slow
2021-12-01 12:11:16.743 | INFO     | __main__:main_worker:216 - Loading checkpoint 'model/resnet18/model_best.pth.tar'
2021-12-01 12:11:16.787 | INFO     | __main__:main_worker:230 - Loaded checkpoint 'model/resnet18/model_best.pth.tar' (epoch 95)
2021-12-01 12:11:16.788 | WARNING  | __main__:main_worker:235 - Changing model's weights according to 'zero_mantissa_to_closest_exponent' with group size: 64
2021-12-01 12:11:16.788 | INFO     | __main__:convert_model:132 - Converting conv1.weight ...
2021-12-01 12:11:17.009 | INFO     | __main__:convert_model:118 - Skipping bn1.weight ...
2021-12-01 12:11:17.010 | INFO     | __main__:convert_model:118 - Skipping bn1.bias ...
2021-12-01 12:11:17.010 | INFO     | __main__:convert_model:132 - Converting layer1.0.conv1.weight ...
2021-12-01 12:11:18.380 | INFO     | __main__:convert_model:118 - Skipping layer1.0.bn1.weight ...
2021-12-01 12:11:18.381 | INFO     | __main__:convert_model:118 - Skipping layer1.0.bn1.bias ...
2021-12-01 12:11:18.381 | INFO     | __main__:convert_model:132 - Converting layer1.0.conv2.weight ...
2021-12-01 12:11:19.485 | INFO     | __main__:convert_model:118 - Skipping layer1.0.bn2.weight ...
2021-12-01 12:11:19.485 | INFO     | __main__:convert_model:118 - Skipping layer1.0.bn2.bias ...
2021-12-01 12:11:19.485 | INFO     | __main__:convert_model:132 - Converting layer1.1.conv1.weight ...
2021-12-01 12:11:20.612 | INFO     | __main__:convert_model:118 - Skipping layer1.1.bn1.weight ...
2021-12-01 12:11:20.612 | INFO     | __main__:convert_model:118 - Skipping layer1.1.bn1.bias ...
2021-12-01 12:11:20.613 | INFO     | __main__:convert_model:132 - Converting layer1.1.conv2.weight ...
2021-12-01 12:11:21.545 | INFO     | __main__:convert_model:118 - Skipping layer1.1.bn2.weight ...
2021-12-01 12:11:21.546 | INFO     | __main__:convert_model:118 - Skipping layer1.1.bn2.bias ...
2021-12-01 12:11:21.546 | INFO     | __main__:convert_model:132 - Converting layer2.0.conv1.weight ...
2021-12-01 12:11:23.090 | INFO     | __main__:convert_model:118 - Skipping layer2.0.bn1.weight ...
2021-12-01 12:11:23.090 | INFO     | __main__:convert_model:118 - Skipping layer2.0.bn1.bias ...
2021-12-01 12:11:23.091 | INFO     | __main__:convert_model:132 - Converting layer2.0.conv2.weight ...
2021-12-01 12:11:26.159 | INFO     | __main__:convert_model:118 - Skipping layer2.0.bn2.weight ...
2021-12-01 12:11:26.159 | INFO     | __main__:convert_model:118 - Skipping layer2.0.bn2.bias ...
2021-12-01 12:11:26.159 | INFO     | __main__:convert_model:118 - Skipping layer2.0.downsample.0.weight ...
2021-12-01 12:11:26.160 | INFO     | __main__:convert_model:118 - Skipping layer2.0.downsample.1.weight ...
2021-12-01 12:11:26.160 | INFO     | __main__:convert_model:118 - Skipping layer2.0.downsample.1.bias ...
2021-12-01 12:11:26.160 | INFO     | __main__:convert_model:132 - Converting layer2.1.conv1.weight ...
2021-12-01 12:11:29.417 | INFO     | __main__:convert_model:118 - Skipping layer2.1.bn1.weight ...
2021-12-01 12:11:29.417 | INFO     | __main__:convert_model:118 - Skipping layer2.1.bn1.bias ...
2021-12-01 12:11:29.418 | INFO     | __main__:convert_model:132 - Converting layer2.1.conv2.weight ...
2021-12-01 12:11:32.765 | INFO     | __main__:convert_model:118 - Skipping layer2.1.bn2.weight ...
2021-12-01 12:11:32.766 | INFO     | __main__:convert_model:118 - Skipping layer2.1.bn2.bias ...
2021-12-01 12:11:32.766 | INFO     | __main__:convert_model:132 - Converting layer3.0.conv1.weight ...
2021-12-01 12:11:38.849 | INFO     | __main__:convert_model:118 - Skipping layer3.0.bn1.weight ...
2021-12-01 12:11:38.849 | INFO     | __main__:convert_model:118 - Skipping layer3.0.bn1.bias ...
2021-12-01 12:11:38.849 | INFO     | __main__:convert_model:132 - Converting layer3.0.conv2.weight ...
2021-12-01 12:11:51.366 | INFO     | __main__:convert_model:118 - Skipping layer3.0.bn2.weight ...
2021-12-01 12:11:51.366 | INFO     | __main__:convert_model:118 - Skipping layer3.0.bn2.bias ...
2021-12-01 12:11:51.366 | INFO     | __main__:convert_model:118 - Skipping layer3.0.downsample.0.weight ...
2021-12-01 12:11:51.366 | INFO     | __main__:convert_model:118 - Skipping layer3.0.downsample.1.weight ...
2021-12-01 12:11:51.366 | INFO     | __main__:convert_model:118 - Skipping layer3.0.downsample.1.bias ...
2021-12-01 12:11:51.366 | INFO     | __main__:convert_model:132 - Converting layer3.1.conv1.weight ...
2021-12-01 12:12:03.667 | INFO     | __main__:convert_model:118 - Skipping layer3.1.bn1.weight ...
2021-12-01 12:12:03.668 | INFO     | __main__:convert_model:118 - Skipping layer3.1.bn1.bias ...
2021-12-01 12:12:03.668 | INFO     | __main__:convert_model:132 - Converting layer3.1.conv2.weight ...
2021-12-01 12:12:17.166 | INFO     | __main__:convert_model:118 - Skipping layer3.1.bn2.weight ...
2021-12-01 12:12:17.167 | INFO     | __main__:convert_model:118 - Skipping layer3.1.bn2.bias ...
2021-12-01 12:12:17.167 | INFO     | __main__:convert_model:132 - Converting layer4.0.conv1.weight ...
2021-12-01 12:12:41.769 | INFO     | __main__:convert_model:118 - Skipping layer4.0.bn1.weight ...
2021-12-01 12:12:41.770 | INFO     | __main__:convert_model:118 - Skipping layer4.0.bn1.bias ...
2021-12-01 12:12:41.770 | INFO     | __main__:convert_model:132 - Converting layer4.0.conv2.weight ...
2021-12-01 12:13:31.329 | INFO     | __main__:convert_model:118 - Skipping layer4.0.bn2.weight ...
2021-12-01 12:13:31.330 | INFO     | __main__:convert_model:118 - Skipping layer4.0.bn2.bias ...
2021-12-01 12:13:31.330 | INFO     | __main__:convert_model:118 - Skipping layer4.0.downsample.0.weight ...
2021-12-01 12:13:31.330 | INFO     | __main__:convert_model:118 - Skipping layer4.0.downsample.1.weight ...
2021-12-01 12:13:31.330 | INFO     | __main__:convert_model:118 - Skipping layer4.0.downsample.1.bias ...
2021-12-01 12:13:31.330 | INFO     | __main__:convert_model:132 - Converting layer4.1.conv1.weight ...
2021-12-01 12:14:20.707 | INFO     | __main__:convert_model:118 - Skipping layer4.1.bn1.weight ...
2021-12-01 12:14:20.707 | INFO     | __main__:convert_model:118 - Skipping layer4.1.bn1.bias ...
2021-12-01 12:14:20.707 | INFO     | __main__:convert_model:132 - Converting layer4.1.conv2.weight ...
2021-12-01 12:15:10.764 | INFO     | __main__:convert_model:118 - Skipping layer4.1.bn2.weight ...
2021-12-01 12:15:10.765 | INFO     | __main__:convert_model:118 - Skipping layer4.1.bn2.bias ...
2021-12-01 12:15:10.765 | INFO     | __main__:convert_model:132 - Converting fc.weight ...
2021-12-01 12:15:10.907 | INFO     | __main__:convert_model:127 - Converting fc.bias with group size: 10. (W.numel()(=10) < 64) ...
2021-12-01 12:15:10.908 | INFO     | __main__:main_worker:244 - Train Dir: data/imagenette/train | Val Dir: data/imagenette/val
2021-12-01 12:15:10.951 | INFO     | __main__:main_worker:269 - args.evaluate=True: Doing evaluation once, and exit
2021-12-01 12:15:12.335 | INFO     | __main__:display:446 - Test: [  0/123]	Time  1.382 ( 1.382)	Loss 3.9903e-01 (3.9903e-01)	Acc@1  90.62 ( 90.62)	Acc@5  93.75 ( 93.75)
2021-12-01 12:15:20.205 | INFO     | __main__:display:446 - Test: [ 10/123]	Time  0.710 ( 0.841)	Loss 5.8748e-01 (3.3180e-01)	Acc@1  84.38 ( 90.06)	Acc@5  96.88 ( 98.01)
2021-12-01 12:15:27.886 | INFO     | __main__:display:446 - Test: [ 20/123]	Time  0.916 ( 0.806)	Loss 8.1507e-01 (4.6056e-01)	Acc@1  75.00 ( 86.61)	Acc@5  93.75 ( 97.92)
2021-12-01 12:15:35.384 | INFO     | __main__:display:446 - Test: [ 30/123]	Time  0.750 ( 0.788)	Loss 6.4122e-01 (5.3253e-01)	Acc@1  84.38 ( 84.17)	Acc@5  96.88 ( 97.28)
2021-12-01 12:15:43.098 | INFO     | __main__:display:446 - Test: [ 40/123]	Time  0.811 ( 0.784)	Loss 6.4957e-01 (5.5225e-01)	Acc@1  75.00 ( 83.38)	Acc@5 100.00 ( 97.71)
2021-12-01 12:15:51.079 | INFO     | __main__:display:446 - Test: [ 50/123]	Time  0.866 ( 0.787)	Loss 8.8137e-01 (6.1635e-01)	Acc@1  68.75 ( 81.37)	Acc@5  96.88 ( 97.55)
2021-12-01 12:15:58.564 | INFO     | __main__:display:446 - Test: [ 60/123]	Time  0.732 ( 0.781)	Loss 8.7031e-01 (6.1462e-01)	Acc@1  75.00 ( 81.51)	Acc@5  96.88 ( 97.49)
2021-12-01 12:16:06.403 | INFO     | __main__:display:446 - Test: [ 70/123]	Time  0.750 ( 0.781)	Loss 4.0651e-01 (5.9397e-01)	Acc@1  87.50 ( 81.87)	Acc@5 100.00 ( 97.80)
2021-12-01 12:16:14.207 | INFO     | __main__:display:446 - Test: [ 80/123]	Time  0.748 ( 0.781)	Loss 1.0047e+00 (5.8232e-01)	Acc@1  68.75 ( 82.25)	Acc@5  96.88 ( 97.84)
2021-12-01 12:16:22.395 | INFO     | __main__:display:446 - Test: [ 90/123]	Time  0.842 ( 0.785)	Loss 5.8334e-01 (5.8313e-01)	Acc@1  87.50 ( 82.18)	Acc@5 100.00 ( 97.97)
2021-12-01 12:16:30.087 | INFO     | __main__:display:446 - Test: [100/123]	Time  0.761 ( 0.784)	Loss 1.5149e+00 (6.2272e-01)	Acc@1  59.38 ( 81.06)	Acc@5  93.75 ( 97.74)
2021-12-01 12:16:37.966 | INFO     | __main__:display:446 - Test: [110/123]	Time  0.863 ( 0.784)	Loss 8.2735e-01 (6.7901e-01)	Acc@1  78.12 ( 79.62)	Acc@5  93.75 ( 97.30)
2021-12-01 12:16:45.253 | INFO     | __main__:display:446 - Test: [120/123]	Time  0.717 ( 0.779)	Loss 4.8393e-01 (6.5638e-01)	Acc@1  84.38 ( 80.19)	Acc@5 100.00 ( 97.39)
2021-12-01 12:16:46.515 | INFO     | __main__:display_summary:451 -  *   Acc@1 80.153 Acc@5 97.376
